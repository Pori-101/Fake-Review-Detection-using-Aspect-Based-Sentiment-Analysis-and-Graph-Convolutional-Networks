{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Datasets/ottdata.csv') #Change Path to test code on rest of the datasets\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_texts = df[df['review'].apply(lambda x: not x.strip())]  # This checks for empty or whitespace-only strings\n",
    "\n",
    "print(\"Number of empty text rows:\", len(empty_texts))\n",
    "if not empty_texts.empty:\n",
    "    print(\"Empty text rows:\")\n",
    "    print(empty_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading SenticNet7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_sentic_word_from_excel(filename):\n",
    "    \"\"\" Load SenticNet data from an Excel file into a dictionary. \"\"\"\n",
    "    senticNet = {}\n",
    "    df = pd.read_excel(filename)\n",
    "    for index, row in df.iterrows():\n",
    "        word = row['CONCEPT']\n",
    "        score = row['POLARITY INTENSITY']\n",
    "        senticNet[word] = score\n",
    "    return senticNet\n",
    "\n",
    "# Load SenticNet from an Excel file\n",
    "senticNet = load_sentic_word_from_excel('/Users/prathanaphukon/Desktop/Complete Final Code/senticnet7.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the weighted Adjacency Matrices for each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def dependency_adj_matrix(review, nlp, senticNet):\n",
    "    \"\"\" Create adjacency matrix for dependency and sentiment from given review text. \"\"\"\n",
    "    doc = nlp(review)\n",
    "    tokens = [token for token in doc if not token.is_punct and not token.is_space]\n",
    "    seq_len = len(tokens)\n",
    "    matrix = np.zeros((seq_len, seq_len), dtype='float32')\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_sentic = senticNet.get(token.lemma_.lower(), 0)\n",
    "        token_is_aspect = 1 if token.pos_ == 'NOUN' else 0\n",
    "\n",
    "        for child in token.children:\n",
    "            if child in tokens:\n",
    "                j = tokens.index(child)\n",
    "                child_sentic = senticNet.get(child.lemma_.lower(), 0)\n",
    "                child_is_aspect = 1 if child.pos_ == 'NOUN' else 0\n",
    "\n",
    "                dij = 1  # There's a direct dependency\n",
    "                sij = token_sentic + child_sentic\n",
    "                tij = 1 if (token_is_aspect or child_is_aspect) else 0\n",
    "\n",
    "                adjacency_value = dij * (sij + tij + 1)\n",
    "\n",
    "                matrix[i][j] = adjacency_value\n",
    "                matrix[j][i] = adjacency_value\n",
    "\n",
    "    np.fill_diagonal(matrix, 1)\n",
    "    return matrix\n",
    "\n",
    "# Assuming df['Review'] has been populated\n",
    "df['adjacency_matrix'] = df['review'].apply(lambda review: dependency_adj_matrix(review, nlp, senticNet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the adjacency matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save embeddings to a pickle file\n",
    "with open('adjacency_matrices_ott.pkl', 'wb') as f:\n",
    "    pickle.dump(df['adjacency_matrix'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings from a pickle file\n",
    "with open('adjacency_matrices_ott.pkl', 'rb') as f:\n",
    "    adjacency_matrices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few adjacency matrices to understand their structure\n",
    "print(\"Sample adjacency matrix shapes:\")\n",
    "print(adjacency_matrices.apply(lambda x: np.array(x).shape).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adjacency_matrices.shape)\n",
    "print(adjacency_matrices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming adjacency_matrices is an array of adjacency matrices\n",
    "print(adjacency_matrices.shape)\n",
    "\n",
    "# Process only the first 5 adjacency matrices\n",
    "for idx in range(5):  # Adjust here to select how many matrices to process\n",
    "    adj_matrix = adjacency_matrices[idx]\n",
    "    \n",
    "    # Extract the first 5 rows\n",
    "    first_5_rows = adj_matrix[:5, :]\n",
    "    \n",
    "    # Find the maximum and minimum values in these rows\n",
    "    max_value = np.max(first_5_rows)\n",
    "    min_value = np.min(first_5_rows)\n",
    "    \n",
    "    print(f\"Matrix {idx + 1}: Max value in the first 5 rows is {max_value}\")\n",
    "    print(f\"Matrix {idx + 1}: Min value in the first 5 rows is {min_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove Embeddings for Review Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Glove Embeddings\n",
    "def load_glove_embeddings(path):\n",
    "    embeddings_index = {}\n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(reviews):\n",
    "    \"\"\"Tokenizes reviews and returns a list of lists of token texts.\"\"\"\n",
    "    tokenized_reviews = []\n",
    "    for review in reviews:\n",
    "        doc = nlp(review)\n",
    "        tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
    "        tokenized_reviews.append(tokens)\n",
    "    return tokenized_reviews\n",
    "\n",
    "df['tokenized_reviews'] = tokenize_reviews(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_embeddings(tokens, embeddings_index):\n",
    "    \"\"\"Converts token lists to a list of embeddings\"\"\"\n",
    "    embedded_docs = []\n",
    "    for doc in tokens:\n",
    "        doc_embeddings = []\n",
    "        for word in doc:\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                doc_embeddings.append(embedding_vector)\n",
    "            else:\n",
    "                doc_embeddings.append(np.zeros(300))  # Fill with zeros for missing embeddings\n",
    "        embedded_docs.append(np.array(doc_embeddings))\n",
    "    return embedded_docs\n",
    "\n",
    "# Get embeddings for each token in the reviews\n",
    "df['glove_embedding'] = get_embeddings(df['tokenized_reviews'], glove_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few adjacency matrices to understand their structure\n",
    "print(\"Sample glove embedding shapes:\")\n",
    "print(df['glove_embedding'].apply(lambda x: np.array(x).shape).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['glove_embedding'].shape)\n",
    "print(df['glove_embedding'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save embeddings to a pickle file\n",
    "with open('glove_ott.pkl', 'wb') as f:\n",
    "    pickle.dump(df['glove_embedding'], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings from a pickle file\n",
    "with open('glove_ott.pkl', 'rb') as f:\n",
    "    glove_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove_embeddings.shape)\n",
    "print(glove_embeddings[1].shape)\n",
    "print(glove_embeddings[1])\n",
    "print(glove_embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove_embeddings.shape)  # For DataFrame\n",
    "print(glove_embeddings.head())  # To see the top rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Node Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embeddings (list of torch.Tensor): The list containing GloVe embeddings for each review.\n",
    "            labels (iterable): Corresponding labels for the embeddings.\n",
    "        \"\"\"\n",
    "        self.embeddings = [torch.tensor(e, dtype=torch.float32) if not isinstance(e, torch.Tensor) else e for e in embeddings]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    embeddings, labels = zip(*batch)\n",
    "    lengths = torch.tensor([len(e) for e in embeddings], dtype=torch.long)\n",
    "    embeddings_padded = pad_sequence(embeddings, batch_first=True, padding_value=0.0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return embeddings_padded, labels, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        \n",
    "        # Fully connected layer to transform back to input feature size\n",
    "        self.fc = nn.Linear(hidden_dim * 2, input_dim)  # x2 for bidirection\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack the padded sequence\n",
    "        x_packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Process with LSTM\n",
    "        packed_output, (hidden, cell) = self.lstm(x_packed)\n",
    "\n",
    "        # Unpack sequence\n",
    "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # Pass the output through a linear layer and use tanh to keep the output bounded\n",
    "        output = torch.tanh(self.fc(output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Setup DataLoader and Model\n",
    "# --------------------------\n",
    "# This should match the actual data you have and how your dataset is setup\n",
    "dataset = ReviewDataset(glove_embeddings, labels)  # assuming glove_embeddings and labels are defined\n",
    "dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Define the Bi-LSTM Model\n",
    "model = BiLSTM(input_dim=300, hidden_dim=150, num_layers=2, dropout=0.5)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()  # Move model to GPU if available\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "# ---------------------------\n",
    "criterion = torch.nn.MSELoss()  # This should be chosen based on your specific task\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "# -------------\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    for embeddings_batch, labels_batch, lengths in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            embeddings_batch = embeddings_batch.cuda()  # Move data to GPU if available\n",
    "            labels_batch = labels_batch.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(embeddings_batch, lengths)\n",
    "        \n",
    "        # Compute the loss; \n",
    "        loss = criterion(outputs, embeddings_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    average_loss = total_loss / batch_count\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Collect embeddings without altering their shapes\n",
    "unbatched_embeddings = []\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "with torch.no_grad():\n",
    "    for embeddings_batch, labels_batch, lengths in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            embeddings_batch = embeddings_batch.cuda()\n",
    "        \n",
    "        # Compute embeddings\n",
    "        outputs = model(embeddings_batch, lengths)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            outputs = outputs.cpu()  # If using GPU, transfer outputs back to CPU\n",
    "        \n",
    "        # Append the outputs to the list, respecting the original batch separation\n",
    "        for i in range(len(outputs)):\n",
    "            unbatched_embeddings.append(outputs[i][:lengths[i]])  # Only append up to the original length of each sequence\n",
    "\n",
    "# Now, `unbatched_embeddings` is a list of tensors where each tensor corresponds to an input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and print the shapes of the embeddings to verify\n",
    "for i, emb in enumerate(unbatched_embeddings):\n",
    "    print(f\"Shape of embedding {i + 1}: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the LSTM Node Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the unbatched embeddings to a pickle file\n",
    "with open('unbatched_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(unbatched_embeddings, f)\n",
    "\n",
    "print(\"Unbatched embeddings have been saved to 'unbatched_embeddings.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings from a pickle file\n",
    "with open('unbatched_embeddings.pkl', 'rb') as f:\n",
    "    lstm_embeddings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as a torch object if file is of a larger size\n",
    "\n",
    "# import torch\n",
    "\n",
    "# # Save the embeddings to a file \n",
    "# torch.save(unbatched_embeddings, '40lstm_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Load the embeddings from a file\n",
    "# lstm_embeddings = torch.load('40lstm_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def create_data_objects(adjacency_matrices, loaded_embeddings, labels):\n",
    "    data_list = []\n",
    "\n",
    "    for adj_matrix, node_features, label in zip(adjacency_matrices, lstm_embeddings, labels):\n",
    "        edge_indices = np.transpose(np.nonzero(adj_matrix)).astype(np.int64)\n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        y = torch.tensor([label], dtype=torch.float)  # Make sure label is a tensor\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(df['label'].values, dtype=torch.float)\n",
    "\n",
    "data_list = create_data_objects(adjacency_matrices, lstm_embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_list(data_list):\n",
    "    for i, data in enumerate(data_list):\n",
    "        print(f\"Graph {i}:\")\n",
    "        print(f\"  - Node features shape (x): {data.x.shape}\")  # Shape of the node feature matrix\n",
    "        print(f\"  - Edge index shape (edge_index): {data.edge_index.shape}\")  # Shape of the edge index matrix\n",
    "        print(f\"  - Number of nodes: {data.num_nodes}\")  # Total number of nodes\n",
    "        print(f\"  - Number of edges: {data.num_edges}\")  # Total number of edges\n",
    "        if data.y.numel() == 1:\n",
    "            print(f\"  - Label Value: {data.y.item()}\")  # Use .item() for single-element tensors\n",
    "        else:\n",
    "            print(f\"  - Labels: {data.y}\")  # Print the whole tensor if it contains more than one element\n",
    "        # Print a newline for better separation between graphs\n",
    "        print()\n",
    "\n",
    "# Call the function to print the data shapes and counts\n",
    "verify_data_list(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_list_ott.pkl', 'wb') as f:\n",
    "    pickle.dump(data_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load embeddings from a pickle file\n",
    "with open('data_list_ott.pkl', 'rb') as f:\n",
    "    data_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graph_statistics(data_list):\n",
    "    node_counts = [data.num_nodes for data in data_list]\n",
    "    edge_counts = [data.num_edges for data in data_list]\n",
    "\n",
    "    # Initialize lists for degrees and centralities\n",
    "    all_degrees = []\n",
    "    all_betweenness = []\n",
    "    all_closeness = []\n",
    "    all_degree_centrality = []\n",
    "    all_harmonic_centrality = []\n",
    "\n",
    "    # Iterate over each graph to calculate degrees and centralities\n",
    "    for data in data_list:\n",
    "        G = nx.Graph()  # Assuming undirected graphs; use nx.DiGraph() for directed graphs\n",
    "        for i in range(data.edge_index.shape[1]):  # Number of edges\n",
    "            src = data.edge_index[0][i].item()\n",
    "            dst = data.edge_index[1][i].item()\n",
    "            G.add_edge(src, dst)  # Add edges to the graph\n",
    "        \n",
    "        # Calculate degrees for each node\n",
    "        degrees = [degree for _, degree in G.degree()]\n",
    "        all_degrees.extend(degrees)\n",
    "        \n",
    "        # Calculate betweenness centrality\n",
    "        betweenness = list(nx.betweenness_centrality(G).values())\n",
    "        all_betweenness.extend(betweenness)\n",
    "        \n",
    "        # Calculate closeness centrality\n",
    "        closeness = list(nx.closeness_centrality(G).values())\n",
    "        all_closeness.extend(closeness)\n",
    "        \n",
    "        # Calculate degree centrality\n",
    "        degree_centrality = list(nx.degree_centrality(G).values())\n",
    "        all_degree_centrality.extend(degree_centrality)\n",
    "        \n",
    "        # Calculate harmonic centrality\n",
    "        harmonic_centrality = list(nx.harmonic_centrality(G).values())\n",
    "        all_harmonic_centrality.extend(harmonic_centrality)\n",
    "\n",
    "    # Plot the graphs\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.hist(node_counts, bins=30, color='skyblue')\n",
    "    plt.title('Distribution of Node Counts')\n",
    "    plt.xlabel('Number of Nodes')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.hist(edge_counts, bins=30, color='salmon')\n",
    "    plt.title('Distribution of Edge Counts')\n",
    "    plt.xlabel('Number of Edges')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.hist(all_degrees, bins=30, color='green')\n",
    "    plt.title('Distribution of Node Degrees')\n",
    "    plt.xlabel('Degree')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.hist(all_betweenness, bins=30, color='purple')\n",
    "    plt.title('Betweenness Centrality')\n",
    "    plt.xlabel('Centrality')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.hist(all_closeness, bins=30, color='orange')\n",
    "    plt.title('Closeness Centrality')\n",
    "    plt.xlabel('Centrality')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.hist(all_degree_centrality, bins=30, color='blue')\n",
    "    plt.title('Degree Centrality')\n",
    "    plt.xlabel('Centrality')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_graph_statistics(data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def train_val_test_split(data_list, train_ratio=0.8, val_ratio=0.1):\n",
    "    total_size = len(data_list)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    val_size = int(total_size * val_ratio)\n",
    "    test_size = total_size - train_size - val_size\n",
    "\n",
    "    # Shuffle data indices\n",
    "    indices = np.random.permutation(total_size)\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "\n",
    "    train_dataset = [data_list[i] for i in train_indices]\n",
    "    val_dataset = [data_list[i] for i in val_indices]\n",
    "    test_dataset = [data_list[i] for i in test_indices]\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Assuming data_list is defined as shown in the previous step\n",
    "train_dataset, val_dataset, test_dataset = train_val_test_split(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        # Increasing depth by adding an extra convolutional layer\n",
    "        self.conv1 = GCNConv(num_features, 64)  # First layer with 64 units\n",
    "        self.conv2 = GCNConv(64, 128)  # Second layer with 128 units\n",
    "        self.conv3 = GCNConv(128, 256)  # Third layer with 256 units\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc = torch.nn.Linear(256, num_classes)  # Adjusting the linear layer to match the output of the last GCN layer\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Apply the first GCN layer and ReLU activation\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the second GCN layer and ReLU activation\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply the third GCN layer and ReLU activation\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Global mean pooling to aggregate node features to the graph-level\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Apply the final fully connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            total_loss += criterion(out, data.y.long()).item()\n",
    "            correct += int((pred == data.y).sum())\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "model = GCN(num_features=300, num_classes=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Early Stopping and Training Loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}%')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter == patience:\n",
    "            print(\"Stopping early due to no improvement\")\n",
    "            break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data)\n",
    "            pred = out.argmax(dim=1)\n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            labels.extend(data.y.cpu().numpy())\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "predictions, labels = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(labels, predictions)\n",
    "precision = precision_score(labels, predictions, average='binary')\n",
    "recall = recall_score(labels, predictions, average='binary')\n",
    "f1 = f1_score(labels, predictions, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(labels, predictions):\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "def plot_precision_recall_curve(labels, predictions):\n",
    "    precision, recall, _ = precision_recall_curve(labels, predictions)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Precision-Recall curve (area = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        prob = torch.softmax(out, dim=1)[:, 1]  #Printing Probabilities for class 1\n",
    "        probabilities.extend(prob.cpu().numpy())\n",
    "\n",
    "plot_precision_recall_curve(labels, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import torch\n",
    "\n",
    "def plot_curves(labels, probabilities):\n",
    "    \n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, _ = roc_curve(labels, probabilities)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plotting ROC Curve\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Assuming model and test_loader are defined and properly configured\n",
    "model.eval()\n",
    "probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        out = model(data)\n",
    "        prob = torch.softmax(out, dim=1)[:, 1]  #Printing Probabilities for class 1\n",
    "        probabilities.extend(prob.cpu().numpy())\n",
    "\n",
    "plot_curves(labels, probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
